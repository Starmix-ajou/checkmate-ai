import logging
import os
from collections import defaultdict
from typing import List

import torch
from dotenv import load_dotenv
from gpt_utils import extract_json_from_gpt_response
from huggingface_hub import login
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from mongodb_setting import (get_epic_collection, get_project_collection,
                             get_user_collection)
from openai import AsyncOpenAI
from project_member_utils import get_project_members
from redis_setting import load_from_redis, save_to_redis
from transformers import (AutoModelForTokenClassification, AutoTokenizer,
                          TokenClassificationPipeline)

logger = logging.getLogger(__name__)

load_dotenv(os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env'))
os.environ["TOKENIZERS_PARALLELISM"] = "false"

HUGGINGFACE_API_KEY = os.getenv("HUGGINGFACE_API_KEY")
login(token=HUGGINGFACE_API_KEY)


'''
Tokenì€ BIO í˜•ì‹ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬. BIO í˜•ì‹ì€ ê° í† í°ì— ëŒ€í•´ íƒœê·¸ ë¶€ì—¬ ë°©ì‹ì„ ì •ì˜í•œ ê²ƒ.

B: Begin, I: Inside, O: Outside
B-PER: ì‹œì‘ í† í°, I-PER: ì¤‘ê°„ í† í°, O: ê·¸ ì™¸ í† í°
'''


specify_model_name = "monologg/koelectra-base-v3-naver-ner"
'''
ìŠ¤í™ ì§§ê²Œ ì •ë¦¬: 
    - koelectra-base-v3 (í•œêµ­ì–´ ELECTRA ë³€í˜•)ì´ ê¸°ë°˜ ëª¨ë¸
    - Input: 512 tokens
    - Class: 41ê°œ NER tagì— ëŒ€í•´ í•™ìŠµí•¨
    - tokenizer íŠ¹ì§•: ê³µë°±, ë‹¨ì–´ ê²½ê³„ ë¬´ì‹œ && Sentence ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•¨
    - F1-SCORE: 92.4
    - # of parameters: 110M
    - ì†ë„: ë¹ ë¦„
'''

tokenizer = AutoTokenizer.from_pretrained(specify_model_name)
model_for_ner = AutoModelForTokenClassification.from_pretrained(specify_model_name)

async def create_action_items_finetuned(content: str):
    logger.info(f"ğŸ” íšŒì˜ ì•¡ì…˜ ì•„ì´í…œ ìƒì„± ì‹œì‘")
    
    # ëª¨ë¸ì˜ ì…ë ¥ token ë‹¨ìœ„ê°€ 512ê°œì´ë¯€ë¡œ, ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•´ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    paragraphs = content.split("\n\n")
    entities = []
    
    # ê° ë¬¸ë‹¨ë³„ë¡œ ì²˜ë¦¬
    for paragraph in paragraphs:
        if not paragraph.strip():   # ë¹„ì–´ ìˆëŠ” ë¬¸ë‹¨ì€ ìƒëµ
            continue
        
        # ë¬¸ë‹¨ì„ í† í°í™”
        inputs = tokenizer(
            paragraph,
            return_tensors="pt",
            truncation=True,
            max_length=512,  # ëª¨ë¸ì˜ ì…ë ¥ token ë‹¨ìœ„ê°€ 512ê°œì´ë¯€ë¡œ, ì´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë§ì¶°ì„œ ì²˜ë¦¬
            return_offsets_mapping=True,
            return_token_type_ids=False,
        )
        offset_mapping = inputs.pop("offset_mapping")[0] # offset mappingì„ í•´ì•¼ ì´í›„ì— í† í°ì— ëŒ€ì‘ë˜ëŠ” ë¬¸ìì—´ì„ ë³µì›í•  ìˆ˜ ìˆìŒ
        
        with torch.no_grad():
            outputs = model_for_ner(**inputs)
        
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=2)[0].tolist()   # listë¡œ ë¬¶ì–´ì„œ ì°¨ì› ì¶•ì†Œ
        input_ids = inputs["input_ids"][0]
        id2label = model_for_ner.config.id2label
        tokens = tokenizer.convert_ids_to_tokens(input_ids)
    
        current_entity = {"type": None, "text": "", "start": None}
    
        for i, pred in enumerate(predictions):
            label = id2label[pred]
            start, end = offset_mapping[i].tolist()
            text_span = paragraph[start:end]
        
            if "-" in label:
                entity_type, tag = label.split("-")
            else:
                entity_type, tag = label, "0"
                
            if tag == "B":
                # ì´ì „ entity ì €ì¥
                if current_entity["text"]:
                    entities.append(current_entity.copy())
                current_entity = {
                    "type": entity_type,
                    "text": text_span,
                    "start": start,
                }
            elif tag == "I" and current_entity["type"] == entity_type:
                current_entity["text"] += text_span
            else:
                # Outside íƒœê·¸ / type ë¶ˆì¼ì¹˜
                if current_entity["text"]:
                    entities.append(current_entity.copy())
                    current_entity = {"type": None, "text": "", "start": None}
                    
        if current_entity["text"]:
            # ë§ˆì§€ë§‰ entity ì²˜ë¦¬
            entities.append(current_entity.copy())
    
    # ë””ë²„ê¹…ìš© ì¶œë ¥
    print("=== Extracted Entities ===")
    for ent in entities:
        print(ent)
        
    # taskë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ assignee, enddate mapping
    action_items = []
    tasks = [e for e in entities if e["type"] in {"EVT", "TRM"}]
    assignees = [e for e in entities if e["type"] == "PER"]
    enddates = [e for e in entities if e["type"] == "DAT"]
    
    for task in tasks:
        closest_assignee = min(assignees, key=lambda x: abs(x["start"] - task["start"]), default=None)
        closest_enddate = min(enddates, key=lambda x: abs(x["start"] - task["start"]), default=None)
        
        action_items.append({
            "task": task["text"],
            "assignee": closest_assignee["text"] if closest_assignee else "",
            "enddate": closest_enddate["text"] if closest_enddate else "",
        })
        
    ''' 
    ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¡œ action_itemsê°€ ë°˜í™˜ë˜ë©´ ì„±ê³µ
    [
        {
            "task": "ë³´ê³ ì„œ ì œì¶œ",
            "assignee": "ê¹€ìŠ¹ì—°",
            "enddate": "2024ë…„ 10ì›” 1ì¼"
        },
        {
            "task": "ìë£Œ ì •ë¦¬",
            "assignee": "",
            "enddate": ""
        },
    ]
    '''
    print(f"ìµœì¢… ì²˜ë¦¬ëœ action_items: {action_items}")
    
    return action_items

async def create_action_items_gpt(content: str):
    logger.info(f"ğŸ” íšŒì˜ ì•¡ì…˜ ì•„ì´í…œ ìƒì„± ì‹œì‘")
    
    action_items_prompt = ChatPromptTemplate.from_template("""
    ë‹¹ì‹ ì€ íšŒì˜ë¡ìœ¼ë¡œë¶€í„° ì•¡ì…˜ ì•„ì´í…œì„ ì •ë¦¬í•´ì£¼ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì£¼ìš” ì–¸ì–´ëŠ” í•œêµ­ì–´ì…ë‹ˆë‹¤.
    íšŒì˜ë¡ {content}ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì„¸ ê°€ì§€ ìš”ì†Œë¥¼ í¬í•¨í•œ ì•¡ì…˜ ì•„ì´í…œì„ ì •ì˜í•´ ì£¼ì„¸ìš”:
    1. ì•¡ì…˜ ì•„ì´í…œ ë‚´ìš©
    2. ë‹´ë‹¹ì
    3. ë§ˆê° ê¸°í•œ
    
    ì„¸ ê°€ì§€ ìš”ì†Œ ì¤‘ íšŒì˜ë¡ì— ì •ë³´ê°€ ì—†ëŠ” ìš”ì†ŒëŠ” nullì„ ë°˜í™˜í•´ ì£¼ì„¸ìš”.
    
    ê²°ê³¼ë¥¼ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ ì£¼ì„¸ìš”:
    {{
        "actionItems": [
            {{
                "task": "string",
                "assignee": "string" | null,
                "enddate": "string" | null
            }},
            ...
        ]
    }}
    """)
    messages = action_items_prompt.format(content=content)
    llm = ChatOpenAI(
        model_name="gpt-4o-mini",
        temperature=0.0,
    )
    response = await llm.ainvoke(messages)
    try:
        content = response.content
        try:
            gpt_result = extract_json_from_gpt_response(content)
        except Exception as e:
            logger.error(f"GPT util ì‚¬ìš© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
            raise Exception(f"GPT util ì‚¬ìš© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True) from e
    except Exception as e:
        logger.error(f"GPT API ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise Exception(f"GPT API ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True) from e
    
    action_items = gpt_result["actionItems"]
    print(f"ìƒì„±ëœ ì•¡ì…˜ ì•„ì´í…œ: {action_items}")
    
    return action_items


### ============================== Summary & Action Items Extraction ============================== ###
async def create_summary(title: str, content: str, project_id: str):
    '''
    title: ì‚¬ìš©ìê°€ ì œëª©ìœ¼ë¡œ íšŒì˜ë¡ì„ ëŒ€í‘œí•˜ëŠ” ë‚´ìš©ì„ ì…ë ¥í•œë‹¤ê³  ê°€ì • -> ìš”ì•½ì˜ ì²« ë²ˆì§¸ ë¼ˆëŒ€ë¡œ ì‚¬ìš©
    content: Markdown í˜•íƒœë¡œ ë¬¸ì„œê°€ ì œê³µë¨
    '''
    logger.info(f"ğŸ” íšŒì˜ ìš”ì•½ ìƒì„± ì‹œì‘")
    
    print(f"íšŒì˜ ì œëª©: {title}")
    meeting_summary_prompt = ChatPromptTemplate.from_template("""
    ë‹¹ì‹ ì€ íšŒì˜ë¡ì—ì„œ ì¤‘ìš”í•œ ëŒ€í™” ë‚´ìš©ì„ ì •ë¦¬í•´ ì£¼ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì£¼ìš” ì–¸ì–´ëŠ” í•œêµ­ì–´ì…ë‹ˆë‹¤. ì •ë¦¬í•œ ë‚´ìš©ì€ ë°˜ë“œì‹œ Markdown í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ ì£¼ì„¸ìš”.
    ë‹¹ì‹ ì˜ ì—…ë¬´ëŠ” íšŒì˜ ì œëª©ì¸ {title}ì„ ë°”íƒ•ìœ¼ë¡œ íšŒì˜ë¡ {content}ë¥¼ ë¶„ì„í•˜ì—¬ ì¤‘ìš”í•œ ëŒ€í™” ë‚´ìš©ì„ ì •ë¦¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
    {title}ì€ íšŒì˜ì˜ ì œëª©ìœ¼ë¡œì„œ íšŒì˜ë¡ì—ì„œ ë…¼ì˜ë˜ëŠ” ë‚´ìš©ì„ ëŒ€í‘œí•˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤. ë”°ë¼ì„œ íšŒì˜ë¡ì˜ ë‚´ìš©ì„ ë¶„ì„í•  ë•Œ {title}ì„ ì ê·¹ì ìœ¼ë¡œ ì°¸ì¡°í•˜ì„¸ìš”.
    Headingì´ ìˆëŠ” ê²½ìš°, ìš”ì•½ ê³¼ì •ì—ì„œë„ Heading ë ˆë²¨ì„ ìœ ì§€í•´ ì£¼ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´, íšŒì˜ë¡ì— "## íšŒì˜ ìš”ì•½"ì´ë¼ëŠ” ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°, ìš”ì•½ ê²°ê³¼ì—ë„ "## íšŒì˜ ìš”ì•½"ì´ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤.
    íšŒì˜ë¡ì—ì„œ ì¤‘ìš”í•œ ëŒ€í™” ë‚´ìš©ì„ ì •ë¦¬í•´ ì£¼ì„¸ìš”. ì¤‘ìš”í•œ ëŒ€í™” ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
    - íšŒì˜ ì•ˆê±´
    - ì•ˆê±´ì— ëŒ€í•œ ë…¼ì˜ ê²°ê³¼
    - ë‹¤ìŒ íšŒì˜ ì•ˆê±´
    - ì¤‘ìš”í•œ í”¼ë“œë°± ë° ì˜ê²¬
    
    í˜„ì¬ í”„ë¡œì íŠ¸ì— ì°¸ì—¬ ì¤‘ì¸ ë©¤ë²„ë“¤ì˜ ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
    {project_members}
    ë©¤ë²„ë“¤ ì¤‘ì—ì„œ íŠ¹ì • ì´ë¦„ì„ ê°€ì§„ ë°œí™”ìê°€ ìˆëŠ” ê²½ìš° ë°œí™”ìì˜ ì´ë¦„ê³¼ ë°œí™” ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ë¬¶ì–´ì„œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.
    
    ê²°ê³¼ë¥¼ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:
    {{
        "summary": "Markdown í˜•ì‹ì˜ íšŒì˜ ìš”ì•½ string",
    }}
    """)
    
    project_members = await get_project_members(project_id)
    project_members_str = "\n".join([f"- {member}" for member in project_members])
    
    messages = meeting_summary_prompt.format(
        title=title, 
        content=content,
        project_members=project_members_str)
    
    llm = ChatOpenAI(
        model_name="gpt-4o-mini",
        temperature=0.1,
    )
    response = await llm.ainvoke(messages)
    
    try:
        content = response.content
        try:
            gpt_result = extract_json_from_gpt_response(content)
        except Exception as e:
            logger.error(f"GPT util ì‚¬ìš© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
            raise Exception(f"GPT util ì‚¬ìš© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True) from e
        
    except Exception as e:
        logger.error(f"GPT API ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise Exception(f"GPT API ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True) from e
    
    summary = gpt_result["summary"]
    print(f"íšŒì˜ ìš”ì•½ ê²°ê³¼: {summary}")
    
    return summary


async def analyze_meeting_document(meeting_id: str, title: str, content: str, project_id: str):
    global action_items, summary
    # ì´ˆê¸°í™” í•„ìˆ˜!
    action_items = None
    summary = None
    
    action_items = await create_action_items_gpt(content)
    try:
        await save_to_redis(f"action_items:{project_id}", action_items)
    except Exception as e:
        logger.error(f"action_items ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        raise Exception(f"action_items ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True) from e
    
    summary = await create_summary(title, content, project_id)
    
    # action_itemsì—ì„œ taskë“¤ë§Œ ì¶”ì¶œ
    task_list = [item["task"] for item in action_items]
    
    response = {
        "summary": summary,
        "actionItems": task_list,
    }
    logger.info(f"êµ¬ì„±ëœ response: {response}")
    return response


async def convert_action_items_to_tasks(actionItems: List[str], project_id: str):
    try:
        action_items = await load_from_redis(f"action_items:{project_id}")
    except Exception as e:
        logger.error(f"action_items ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        raise Exception(f"action_items ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True) from e
    
    assert action_items is not None, "ì •ì˜ë˜ì–´ ìˆëŠ” ì „ì—­ ë³€ìˆ˜ action_itemsê°€ ì—†ìŠµë‹ˆë‹¤."
    
    try:
        # ê°€ì§€ê³  ìˆëŠ” action_itemsì—ì„œ actionItemsì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” taskë¥¼ ì œê±°
        tasks_to_remove = [item for item in action_items if item["task"] not in actionItems]
        print(f"ì œê±°í•  task: {tasks_to_remove}")
        for task in tasks_to_remove:
            action_items.remove(task)
        print(f"ìµœì¢… ë‚¨ì€ task: {action_items}")
        assert len(action_items) == len(actionItems), "action_itemsì™€ actionItemsì˜ ê°œìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤."
    except Exception as e:
        logger.error(f"action_items ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        raise Exception(f"action_items ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True) from e
    
    action_items_to_tasks_prompt = ChatPromptTemplate.from_template(
    """
    ë‹¹ì‹ ì€ ì•¡ì…˜ ì•„ì´í…œì„ íƒœìŠ¤í¬ë¡œ ë³€í™˜í•´ ì£¼ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì£¼ìš” ì–¸ì–´ëŠ” í•œêµ­ì–´ì…ë‹ˆë‹¤.
    ë‹¹ì‹ ì˜ ì—…ë¬´ëŠ” ì‘ì—… ë‚´ìš©, ì‘ì—… ë‹´ë‹¹ì, ì‘ì—… ë§ˆê°ê¸°í•œ ì •ë³´ê°€ ë‹´ê²¨ ìˆëŠ” {action_items}ë¥¼ ë°”íƒ•ìœ¼ë¡œ tasksì˜ title, description, assignee, endDate, epicId ì •ë³´ë¥¼ êµ¬ì„±í•´ ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤.
    ë‹¤ìŒì˜ ê³¼ì •ì„ ë”°ë¼ ê° taskì˜ ë‚´ìš©ì„ êµ¬ì„±í•´ ì£¼ì„¸ìš”. ë°˜ë“œì‹œ {action_items}ì— ì¡´ì¬í•˜ëŠ” ëª¨ë“  taskë¥¼ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.
    1. "task" í•„ë“œê°’ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì—…ëª…(title)ì„ ì„¤ì •í•´ ì£¼ì„¸ìš”.
    2. "task" í•„ë“œê°’ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì—… ì„¤ëª…(description)ì„ ì„¤ì •í•´ ì£¼ì„¸ìš”.
    3. "assignee" í•„ë“œê°’ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì—… ë‹´ë‹¹ì(assignee)ë¥¼ ì„¤ì •í•´ ì£¼ì„¸ìš”. ì‘ì—… ë‹´ë‹¹ì ì •ë³´ê°€ ì—†ë‹¤ë©´ descriptionì„ ë°”íƒ•ìœ¼ë¡œ {project_memebers} ì•ˆì—ì„œ ì ì ˆí•œ ë‹´ë‹¹ìë¥¼ ê³¨ë¼ì„œ ì„¤ì •í•´ ì£¼ì„¸ìš”.
    4. "enddate" í•„ë“œê°’ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì—… ë§ˆê°ê¸°í•œ(endDate)ë¥¼ ì„¤ì •í•´ ì£¼ì„¸ìš”. ë§Œì•½ {action_items}ì— "enddate" í•„ë“œê°’ì´ ì—†ë‹¤ë©´ nullì„ ë°˜í™˜í•˜ì„¸ìš”.
    5. 1-4ë²ˆì˜ ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆë‹¤ë©´, {epics_str}ì—ì„œ ì‘ì—… ë‚´ìš©ê³¼ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì•„ ë³´ì´ëŠ” ì—í”½ì˜ ì´ë¦„ì„ ì°¾ì•„ì„œ í•´ë‹¹ ì—í”½ì˜ epicIdë¥¼ epicId í•„ë“œê°’ìœ¼ë¡œ ë°˜í™˜í•´ ì£¼ì„¸ìš”. ë°˜ë“œì‹œ epicId í•„ë“œê°’ì´ ëª…ì‹œë˜ì–´ì•¼ í•˜ë©°, 
    ì œê³µë˜ëŠ” {epics_str}ì€ '- ì—í”½ ì´ë¦„: ì—í”½ ID' í˜•ì‹ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤.
    ê²°ê³¼ë¥¼ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ ì£¼ì„¸ìš”:
    {{
        "tasks": [
            {{
                "title": "string",
                "description": "string",
                "assignee": "string",
                "endDate": "string" | null,
                "epicId": "string"
            }},
            ...
        ]
    }}
    """)
    
    epics = await get_epic_collection().find_many({"projectId": project_id})
    epics_str = "\n".join([f"- {epic['title']}: {epic['_id']}" for epic in epics])
    project_members = await get_project_members(project_id)
    
    messages = action_items_to_tasks_prompt.format(
        action_items=action_items,
        project_members=project_members,
        epics=epics_str
    )
    llm = ChatOpenAI(
        model_name="gpt-4o-mini",
        temperature=0.3,
    )
    response = await llm.ainvoke(messages)
    try:
        content = response.content
        try:
            gpt_result = extract_json_from_gpt_response(content)
        except Exception as e:
            logger.error(f"GPT util ì‚¬ìš© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
            raise Exception(f"GPT util ì‚¬ìš© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True) from e
    except Exception as e:
        logger.error(f"GPT API ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise Exception(f"GPT API ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True) from e
    
    response = gpt_result["tasks"]
    print(f"íƒœìŠ¤í¬ ê²°ê³¼: {response}")
    
    return response


### ============================== í…ŒìŠ¤íŠ¸ ì½”ë“œ ============================== ###
async def test_meeintg_analysis():
    with open('mvp/meeting_sample.md', 'r', encoding='utf-8') as f:
        content = f.read()
    
    # í…ŒìŠ¤íŠ¸ìš© project_id ì„¤ì •
    project_id = "815cf1fa-2c17-44e5-bd0c-4b93832f67ee"
    
    # ì•¡ì…˜ ì•„ì´í…œ ìƒì„± í…ŒìŠ¤íŠ¸
    logger.info("=== ì•¡ì…˜ ì•„ì´í…œ ìƒì„± í…ŒìŠ¤íŠ¸ ===")
    action_items = await create_action_items_finetuned(content)
    print(f"ìƒì„±ëœ ì•¡ì…˜ ì•„ì´í…œ: {action_items}")
    
    # íšŒì˜ ìš”ì•½ ìƒì„± í…ŒìŠ¤íŠ¸
    logger.info("\n=== íšŒì˜ ìš”ì•½ ìƒì„± í…ŒìŠ¤íŠ¸ ===")
    title = "MVP ê¸°ëŠ¥ ë²”ìœ„ ë° ê°œë°œ ì¼ì • ë…¼ì˜"
    summary = await create_summary(title, content, project_id)
    print("\nìƒì„±ëœ íšŒì˜ ìš”ì•½:")
    print(summary)
    
if __name__ == "__main__":
    #print(model_for_ner.config.id2label)
    import asyncio
    asyncio.run(test_meeintg_analysis())